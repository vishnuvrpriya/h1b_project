import java.io.IOException;



import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.LongWritable.DecreasingComparator;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;

import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Partitioner;
import org.apache.hadoop.mapreduce.Reducer;



public class DataJob {

	
public static class DataJobMapper extends Mapper<LongWritable,Text,Text,LongWritable>
	
	{
		public void map(LongWritable key, Text value, Context context) throws IOException,InterruptedException
		{
			
			
			String h1bdata[] = value.toString().split("\t");
			
			String year = h1bdata[7];
			String jobtitle = h1bdata[4];
			
			if(h1bdata[4].equalsIgnoreCase("DATA ENGINEER"))			
			{
			String myKey = year + "," + jobtitle;
			
			context.write(new Text(myKey),new LongWritable(1));
			
			}
			
		}
	}



public static class DataJobReducer extends Reducer<Text,LongWritable,Text,LongWritable>
{
		String years[] = {"2011","2012","2013","2014","2015","2016"};
		
		int i = 0;
		
		int growthcycles = 0;
		
		long growthval = 0;
		
		long dejobinfo [] = new long[6];
		
		public void reduce(Text inpkey, Iterable<LongWritable> inpvalue, Context context) throws IOException,InterruptedException
		{
			long jobcount = 0;
			
			for(LongWritable eachValue : inpvalue)
			{
				jobcount+=eachValue.get();
			}
				
				dejobinfo[i++] = jobcount;
		
			

			//context.write(inpkey, new LongWritable(jobcount));
		}
		
		public void cleanup(Context context) throws IOException,InterruptedException
		{
			for(i = 0; i < 6; i++)
			{
				if(i==0)
				{
					context.write(new Text(years[i]), new LongWritable(0));
				}
				
				else
				{
					
					growthval = (dejobinfo[i]-dejobinfo[i-1])*100/(dejobinfo[i-1]);
				
					//growthcycles++;
				
					//long avggrowthpercycle = avgval/growthcycles;
				
					context.write(new Text(years[i]), new LongWritable (growthval));
				}
			}
		}
					
					
			}

	
public static void main(String[] args) throws IOException,InterruptedException,ClassCastException,ClassNotFoundException
{
	Configuration conf = new Configuration();
	conf.set("mapreduce.textOutputformat.seperator","\t");
	Job job = Job.getInstance(conf);
	job.setJarByClass(DataJob.class);
	job.setMapperClass(DataJobMapper.class);
	job.setReducerClass(DataJobReducer.class);
	job.setNumReduceTasks(1);
	job.setMapOutputKeyClass(Text.class);
	job.setMapOutputValueClass(LongWritable.class);
	job.setOutputKeyClass(Text.class);
	job.setOutputValueClass(LongWritable.class);
	
	FileInputFormat.addInputPath(job,new Path(args[0]));
	FileOutputFormat.setOutputPath(job,new Path(args[1]));
	
	job.waitForCompletion(true);
}
}


